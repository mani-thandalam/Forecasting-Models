package com.walmart.platform.galileo

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, SparkSession}


object SpAdsDailyAurJob {

  val gcsAurAggregated = "gs://ads-daily_v2_aurdev/"

  def printDataFrameInfo(df: DataFrame, name: String, isDebugMode: Boolean): Unit = {
    if (isDebugMode) {
      println(name + " count : " + df.count())
      println(name + " Schema")
      df.printSchema()
      println(name + " Data")
      df.show(50)
    }
  }

  def createCountsDf(date: String, tenant: String, spark: SparkSession, platforms: Seq[String]): DataFrame = {
    println("Into CreateCounts Df method")
    import spark.implicits._
    var wpaClicks: DataFrame = null
    val wpaClickssql = s"select line from wpa.wpa_clicks_secor where dt = '$date'"
    val schema = new StructType()
      .add("vtc", StringType, true)
      .add("bstc", StringType, true)
      .add("vid", StringType, true)
      .add("sid", StringType, true)
      .add("customerId", StringType, true)
      .add("platform", StringType, false)
      .add("moduleLocation", StringType, true)
      .add("pageType", StringType, true)
      .add("finalCpc", DoubleType, true)
      .add("activityItems", StringType, false)
      .add("bot", BooleanType, false)
      .add("stale", BooleanType, false)
      .add("free", BooleanType, false)
    var colSeq = Seq("")
    if (tenant == "gm_web") {
      colSeq = Seq("vtc", "bstc", "customerId", "moduleLocation", "pageType", "finalCpc", "itemId")
    } else {
      colSeq = Seq("vid", "sid", "customerId", "moduleLocation", "pageType", "finalCpc", "itemId")
    }

    if (tenant == "gm_web") {
      wpaClicks = spark.sql(wpaClickssql).withColumn("line", from_json($"line", schema))
        .select($"line.*").withColumn("itemId", element_at(from_json(col("activityItems"), ArrayType(MapType(StringType, StringType))), 1)("itemId"))
        .where($"bot" === false && $"free" === false && $"stale" === false && $"platform".isin(platforms: _*))
        .select(colSeq.map(col): _*).withColumn("clickcount", lit(1))
    } else {
      wpaClicks = spark.sql(wpaClickssql).withColumn("line", from_json($"line", schema))
        .select($"line.*").withColumn("itemId", element_at(from_json(col("activityItems"), ArrayType(MapType(StringType, StringType))), 1)("itemId"))
        .where($"bot" === false && $"free" === false && $"stale" === false && $"platform".isin(platforms: _*))
        .select(colSeq.map(col): _*).withColumn("clickcount", lit(1))
    }
    wpaClicks
  }


  def createGalileoSessionDailyTotalsDF(spark: SparkSession, date: String, tenant: String): DataFrame = {
    import spark.implicits._
    val galileoSessionDaillyTotalsSQL = spark.sql(s"select vtc, bstc, exp_id, var_id, version, experience_lvl2, (qualified_request or qualified_beacon) as is_qualified, " +
      s"unit_type from expfw.galileo_session_daily_totals_v where tenant = '$tenant' and partition_day = '$date'")
    val expoTenant = if (tenant == "gm_web") "WALMART_US_WWW" else "USGM_MOBILE_PROD"
    val distExpoExperimentSQL = spark.sql(s"select * from expfw.expo_experiments where partition_day = '$date' and tenant = '$expoTenant'")
    val sessionFinalDF = galileoSessionDaillyTotalsSQL.as("se")
      .join(distExpoExperimentSQL.as("ex"),
        $"se.exp_id" === $"ex.exp_spec" && $"se.var_id" === $"ex.var_spec", "inner")
      .select($"se.vtc", $"se.bstc", $"se.exp_id", $"se.var_id", $"se.version", $"ex.is_control", $"se.experience_lvl2", when($"se.is_qualified" === true, 1).otherwise(0) as "is_qualified", $"se.unit_type")
    sessionFinalDF
  }


  def createAggregatedWpaClicksDF(spark: SparkSession, webClicksDF: DataFrame, unit: String, sessionIdColumnName: String): DataFrame = {
    import spark.implicits._
    val moduleLocationPageTypeUnitAggDF = webClicksDF
      .groupBy(col(unit), col(sessionIdColumnName), $"moduleLocation", $"pageType", $"itemId")
      .agg(sum($"finalCpc") as "totalCpc", sum($"clickCount") as "totalClicks")
    val overallUnitAggregatedDF = webClicksDF
      .groupBy(col(unit), col(sessionIdColumnName), $"itemId")
      .agg(sum($"finalCpc") as "totalCpc",  sum($"clickCount") as "totalClicks")
      .withColumn("moduleLocation", lit("overall"))
      .withColumn("pageType", lit("overall"))
    moduleLocationPageTypeUnitAggDF.unionByName(overallUnitAggregatedDF)
      .select(col(unit), col(sessionIdColumnName), $"moduleLocation", $"pageType", $"itemId", $"totalCpc", $"totalClicks")
  }

  def joinGalileoSessionsAndWpaClicks(spark: SparkSession, galileoSessionDailyDF: DataFrame, wpaClicksDF: DataFrame, tenant: String): DataFrame = {
    import spark.implicits._
    val wpaClicksVisitorColumn = if (tenant == "gm_web") "vtc" else "vid"
    val wpaClicksSessionColumn = if (tenant == "gm_web") "bstc" else "sid"
    val visitorAggWpaClicksDF = createAggregatedWpaClicksDF(spark, wpaClicksDF, wpaClicksVisitorColumn, wpaClicksSessionColumn)
    val customerAggWpaClicksDF = createAggregatedWpaClicksDF(spark, wpaClicksDF, "customerId", wpaClicksSessionColumn)
    val nonCIDBasedDF = galileoSessionDailyDF.as("g")
      .filter($"g.unit_type" =!= "cid")
      .join(
        visitorAggWpaClicksDF.as("w"),
        $"g.vtc" === $"w.$wpaClicksVisitorColumn" && $"g.bstc" === $"w.$wpaClicksSessionColumn",
        "inner")
      .select($"g.vtc", $"g.bstc", $"g.exp_id", $"g.var_id", $"g.version", $"g.is_control", $"g.experience_lvl2", $"g.is_qualified", $"w.moduleLocation", $"w.pageType", $"w.totalCpc",
        $"w.totalClicks",$"w.itemId")
    val cidBasedDF = galileoSessionDailyDF.as("g")
      .filter($"g.unit_type" === "cid")
      .join(
        customerAggWpaClicksDF.as("w"),
        $"g.vtc" === $"w.customerId" && $"g.bstc" === $"w.$wpaClicksSessionColumn",
        "inner")
      .select($"g.vtc", $"g.bstc", $"g.exp_id", $"g.var_id", $"g.version", $"g.is_control", $"g.experience_lvl2", $"g.is_qualified", $"w.moduleLocation", $"w.pageType", $"w.totalCpc",
        $"w.totalClicks",$"w.itemId")
    nonCIDBasedDF.unionByName(cidBasedDF).toDF()
  }


  def vtcRollUp (spark: SparkSession, galileoSessionWithWpaClicksDF: DataFrame): DataFrame = {
    import spark.implicits._

    //roll up to vtc level here
    val currDailyRolledUp = galileoSessionWithWpaClicksDF
      .as("ses")
      .groupBy(
        $"ses.vtc",
        $"ses.exp_id",
        $"ses.var_id",
        $"ses.version",
        $"ses.is_control",
        $"ses.experience_lvl2",
        $"ses.moduleLocation",
        $"ses.itemId",
        $"ses.pageType")
      .agg(
        sum($"totalCpc") as "cpc",
        sum($"totalClicks") as "clicks",
        max($"is_qualified") as "is_qualified")

    //Adding overall
    val currDailyRolledUpWithOverAll = galileoSessionWithWpaClicksDF
      .as("ses")
      .groupBy(
        $"ses.vtc",
        $"ses.exp_id",
        $"ses.var_id",
        $"ses.version",
        $"ses.is_control",
        $"ses.moduleLocation",
        $"ses.itemId",
        $"ses.pageType")
      .agg(
        sum($"totalCpc") as "cpc",
        sum($"totalClicks") as "clicks",
        max($"is_qualified") as "is_qualified")

      .select(
        $"ses.vtc",
        $"ses.exp_id",
        $"ses.var_id",
        $"ses.version",
        $"ses.is_control",
        $"ses.itemId",
        lit("ALL").alias("experience_lvl2"),
        $"clicks",
        $"cpc",
        $"is_qualified",
        $"ses.moduleLocation",
        $"ses.pageType")

    val currDailyAggregated = currDailyRolledUp
      .unionByName(currDailyRolledUpWithOverAll)

    //changing camel case to  hive supported format
    currDailyAggregated.select($"vtc",
      $"exp_id",
      $"var_id",
      $"version",
      $"is_control",
      $"experience_lvl2",
      $"ses.moduleLocation" as "module_location",
      $"ses.pageType" as "page_type",
      $"ses.itemId" as "itemId",
      $"cpc",
      $"clicks",
      $"is_qualified")
  }

  def writeToBucket (spark: SparkSession, aggregatedTable: DataFrame, tenant: String, date: String, location: String) = {
    aggregatedTable
      .withColumn ("tenant", lit (tenant) )
      .withColumn ("partition_day", lit (date) )
      .write.option ("header", true)
      .mode ("overwrite")
      .parquet (location + s"/tenant=$tenant/partition_day=$date")
  }

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder().enableHiveSupport().getOrCreate()

    if (args.length != 2 && args.length != 3) {
      println(args.length)
      throw new IllegalArgumentException("tenant, day [debug]")
    }

    val date = args(0)
    val tenant = args(1)
    val deviceTypes: Seq[String] = if (tenant == "gm_web") Seq("desktop", "mobile") else Seq("mobiApp")
    var isDebugMode = false
    if (args.length == 3 && args(3).equals("debug")) {
      isDebugMode = true
    }

    println(s"Querying WPA Clicks table for tenant - $tenant, date $date and devices $deviceTypes")
    val wpaDF = createCountsDf(date , tenant , spark,  deviceTypes)
    wpaDF.cache()
    printDataFrameInfo(wpaDF, s"WPA Clicks table for tenant - $tenant, date $date and devices $deviceTypes", isDebugMode)

    println(s"Querying GPA Sessions DailyTotal  table for tenant - $tenant, date $date")
    val galileoSessionDailyDF = createGalileoSessionDailyTotalsDF(spark, date, tenant).cache()
    printDataFrameInfo(galileoSessionDailyDF, s"GPA Sessions DailyTotal  table for tenant - $tenant, date $date", isDebugMode)


    println("Inner Join  Sessions Daily table with WPA clicks")
    val galileoSessionWithWpaClicksDF = joinGalileoSessionsAndWpaClicks(spark, galileoSessionDailyDF, wpaDF, tenant)
    galileoSessionWithWpaClicksDF.cache()
    printDataFrameInfo(galileoSessionWithWpaClicksDF, "Galileo Sessions with WPA clicks", isDebugMode)

    //roll up to VTC level
    println("Rolling up at VTC level")
    val vtcRollUpDF = vtcRollUp(spark, galileoSessionWithWpaClicksDF)
    printDataFrameInfo(vtcRollUpDF, "vtcRollUp DF", isDebugMode)

    println("Write daily to Bucket")
    writeToBucket(spark, vtcRollUpDF, tenant, date, gcsAurAggregated)
  }

}
